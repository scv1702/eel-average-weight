{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "eel-average-weight.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scv1702/eel-average-weight/blob/master/eel_average_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 스마트 양식장 뱀장어 평균 중량 인식모델 개발 온라인 해커톤\n",
        "스마트 양식장 수조 내 뱀장어 이미지를 인식하여 해당 수조 내 뱀장어의 평균 중량을 인식하는 모델 개발하기\n",
        "\n",
        "## 대회 주제 및 목표\n",
        "- 주제 : 스마트 양식장 수조 내 이미지를 활용하여 뱀장어의 중량을 인식하는 인공지능 모델 개발\n",
        "\n",
        "## 주최/주관\n",
        "- 주최 : 과학기술정보통신부, 한국지능정보사회진흥원(NIA), 한국판뉴딜\n",
        "- 주관 : 명선해양산업 주식회사, 주식회사 아이싸이랩\n",
        "- 운영 : 인공지능팩토리\n",
        "\n",
        "작성자: 신찬규 (scv1702@naver.com)\n"
      ],
      "metadata": {
        "id": "9P2zF9wIJAm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 구글 드라이브 마운트"
      ],
      "metadata": {
        "id": "W2mviUQrFSe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E83K9xX5FYeh",
        "outputId": "bb971ae6-cd29-44df-e65e-9a7c6a1abf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모듈"
      ],
      "metadata": {
        "id": "o3xawpQjFLbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image as pilimg\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense , Conv2D, Activation, MaxPooling2D , GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-02-09T01:26:32.123955Z",
          "iopub.execute_input": "2022-02-09T01:26:32.124683Z",
          "iopub.status.idle": "2022-02-09T01:26:32.133461Z",
          "shell.execute_reply.started": "2022-02-09T01:26:32.124634Z",
          "shell.execute_reply": "2022-02-09T01:26:32.131939Z"
        },
        "trusted": true,
        "id": "s85mkSxzFDwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경 변수"
      ],
      "metadata": {
        "id": "CR0MmV7JFMvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/drive/MyDrive/eel-average-weight/dataset/train/\"\n",
        "TRAIN_RESIZE_PATH = \"/content/drive/MyDrive/eel-average-weight/dataset/train_resize/\"\n",
        "\n",
        "TEST_PATH = \"/content/drive/MyDrive/eel-average-weight/dataset/test/\"\n",
        "TEST_RESIZE_PATH = \"/content/drive/MyDrive/eel-average-weight/dataset/test_resize/\"\n",
        "\n",
        "TRAIN_LABEL_PATH = \"/content/drive/MyDrive/eel-average-weight/dataset/train.csv\"\n",
        "\n",
        "RESULT_PATH = \"/content/drive/MyDrive/eel-average-weight/result/\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/eel-average-weight/model/\"\n",
        "\n",
        "IMAGE_SIZE = 240\n",
        "\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-09T01:26:32.137704Z",
          "iopub.execute_input": "2022-02-09T01:26:32.139386Z",
          "iopub.status.idle": "2022-02-09T01:26:32.147760Z",
          "shell.execute_reply.started": "2022-02-09T01:26:32.139306Z",
          "shell.execute_reply": "2022-02-09T01:26:32.146290Z"
        },
        "trusted": true,
        "id": "kRHx9LXpFDwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 폴더 생성\n",
        "가중치와 예측 결과를 저장할 폴더 생성"
      ],
      "metadata": {
        "id": "qknaAzvkGLLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(RESULT_PATH)\n",
        "os.mkdir(MODEL_PATH)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pMaXEHPqFDwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 유틸리티"
      ],
      "metadata": {
        "id": "XgssFSXRIjq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete useless weights\n",
        "def delete_weight():\n",
        "    files = os.listdir(MODEL_PATH)\n",
        "    files = [x for x in files if \".hdf5\" in x]\n",
        "\n",
        "    for f in files:\n",
        "        if \".hdf5\":\n",
        "            os.remove(MODEL_PATH + f)\n",
        "\n",
        "    print(\"delete weights complete\")\n",
        "\n",
        "# clean memory\n",
        "def clean_memory():\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(\"clean memory complete \", gc.collect())"
      ],
      "metadata": {
        "id": "UjqhHewLImGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete_weight()\n",
        "clean_memory()"
      ],
      "metadata": {
        "id": "EjsyaHc4NsAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 전처리\n",
        "1920x1080 이미지를 240x240으로 크기 변경"
      ],
      "metadata": {
        "id": "HFwF7HrGGlMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resize images to (IMAGE_SIZE, IMAGE_SIZE)\n",
        "def resize_data(data):\n",
        "    if data == \"train\":\n",
        "        RESIZE_PATH = TRAIN_RESIZE_PATH\n",
        "        ORIGINAL_PATH = TRAIN_PATH\n",
        "    elif data == \"test\":\n",
        "        RESIZE_PATH = TEST_RESIZE_PATH\n",
        "        ORIGINAL_PATH = TEST_PATH\n",
        "\n",
        "    dirs = os.listdir(ORIGINAL_PATH)\n",
        "\n",
        "    for i in range(len(dirs)):\n",
        "        imgs = os.listdir(ORIGINAL_PATH + dirs[i] + \"/\")\n",
        "        imgs = [x for x in imgs if \".jpg\" in x]\n",
        "\n",
        "        for j in range(len(imgs)):\n",
        "            IMG_PATH = ORIGINAL_PATH + dirs[i] + \"/\" + imgs[j]\n",
        "            img = cv2.imread(IMG_PATH)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            resized_img = cv2.resize(img, dsize=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "            cv2.imwrite(RESIZE_PATH + dirs[i] + \"/\" + imgs[j], resized_img)\n",
        "\n",
        "    print(f\"{data} resizing [{i+1}/{len(dirs)}] complete\")"
      ],
      "metadata": {
        "id": "vJF1hanBG5jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_data(\"train\")\n",
        "resize_data(\"test\")"
      ],
      "metadata": {
        "id": "pSihkb8FNxlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageDataGenerator를 위한 DataFrame 생성"
      ],
      "metadata": {
        "id": "owS8dPedHBIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tr_val_df():\n",
        "    train_label_csv = pd.read_csv(TRAIN_LABEL_PATH).sort_values(by=['ImageDir'], axis=0)\n",
        "    train_label_csv = np.array(train_label_csv['AvgWeight'])\n",
        "\n",
        "    dirs = sorted(os.listdir(TRAIN_PATH))\n",
        "\n",
        "    path = []\n",
        "    label = []\n",
        "\n",
        "    for i in range(0, len(dirs), 1):\n",
        "        dir = sorted(os.listdir(TRAIN_PATH + dirs[i] + \"/\"))\n",
        "        for j in range(0, len(dir), 1):\n",
        "            ext = os.path.splitext(dir[j])[-1]\n",
        "            if ext == \".jpg\":\n",
        "                IMG_PATH = TRAIN_PATH + dirs[i] + \"/\" + dir[j]\n",
        "                path.append(IMG_PATH)\n",
        "                label.append(train_label_csv[i])\n",
        "\n",
        "    dataset_df = pd.DataFrame({'path': path, 'label': label})\n",
        "\n",
        "    tr_df, val_df = train_test_split(dataset_df, test_size=0.15, stratify=dataset_df['label'], random_state=2022)\n",
        "    \n",
        "    return tr_df, val_df\n",
        "\n",
        "def get_test_df():\n",
        "    dirs = sorted(os.listdir(TEST_PATH))\n",
        "    path = []\n",
        "    label = []\n",
        "    img_dir = []\n",
        "\n",
        "    for i in range(0, len(dirs), 1):\n",
        "        dir = sorted(os.listdir(TEST_PATH + dirs[i] + \"/\"))\n",
        "        for j in range(0, len(dir), 1):\n",
        "            ext = os.path.splitext(dir[j])[-1]\n",
        "            if ext == \".jpg\":\n",
        "                IMG_PATH = TEST_PATH + dirs[i] + \"/\" + dir[j]\n",
        "                path.append(IMG_PATH)\n",
        "                label.append(\"0\")\n",
        "                img_dir.append(dir[j])\n",
        "\n",
        "    test_df = pd.DataFrame({'path': path, 'ImageDir': img_dir, 'label': label})\n",
        "    \n",
        "    return test_df"
      ],
      "metadata": {
        "id": "D7j3K2dPHHzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 생성"
      ],
      "metadata": {
        "id": "iy93QyrXHMwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(verbose=False):\n",
        "    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        " \n",
        "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    output = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(), loss='mse') \n",
        "\n",
        "    if verbose:\n",
        "        model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-09T01:26:32.394148Z",
          "iopub.execute_input": "2022-02-09T01:26:32.394753Z",
          "iopub.status.idle": "2022-02-09T01:26:32.443508Z",
          "shell.execute_reply.started": "2022-02-09T01:26:32.394716Z",
          "shell.execute_reply": "2022-02-09T01:26:32.442227Z"
        },
        "trusted": true,
        "id": "LscGGtaLFDwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(verbose=True)"
      ],
      "metadata": {
        "id": "iB0fm67XHm0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습 수행"
      ],
      "metadata": {
        "id": "0-dFS13XHtwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, verbose=True):\n",
        "    # dataframe\n",
        "    tr_df, val_df = get_tr_val_df()\n",
        "\n",
        "    # setting image data generators\n",
        "    tr_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "    val_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "    tr_flow_gen = tr_gen.flow_from_dataframe(dataframe=tr_df,\n",
        "                                            x_col='path',\n",
        "                                            y_col='label',\n",
        "                                            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                            class_mode='raw',\n",
        "                                            shuffle=True)\n",
        "\n",
        "    val_flow_gen = val_gen.flow_from_dataframe(dataframe=val_df,\n",
        "                                            x_col='path',\n",
        "                                            y_col='label',\n",
        "                                            target_size=(\n",
        "                                            IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                            class_mode='raw',\n",
        "                                            shuffle=False)\n",
        "\n",
        "    # callbacks\n",
        "    rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5,mode='min', verbose=1)\n",
        "    ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
        "    mcp_cb = ModelCheckpoint(filepath=MODEL_PATH + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "                            monitor='val_loss', save_best_only=True,\n",
        "                            save_weights_only=True, mode='min',\n",
        "                            period=5, verbose=1)\n",
        "\n",
        "    # train the model\n",
        "    history = model.fit(tr_flow_gen, epochs=EPOCHS,\n",
        "                        validation_data=val_flow_gen,\n",
        "                        callbacks=[rlr_cb, ely_cb, mcp_cb],\n",
        "                        verbose=verbose)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "4nOORqFOHv65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_model(model)"
      ],
      "metadata": {
        "id": "PIjPWGuQH2OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 결과 출력"
      ],
      "metadata": {
        "id": "WqGgs34AH4qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_history(history):\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    plt.yticks(np.arange(0, 1, 0.05))\n",
        "    plt.xticks(np.arange(0, EPOCHS, 2))\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='valid')\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "Ru9irCdlH508"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_history(history)"
      ],
      "metadata": {
        "id": "4dqDrBkAH7LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측 수행"
      ],
      "metadata": {
        "id": "aZc71jmMIBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model):\n",
        "    test_df = get_test_df()\n",
        "\n",
        "    test_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "    test_flow_gen = test_gen.flow_from_dataframe(dataframe=test_df,\n",
        "                                                x_col='path',\n",
        "                                                y_col='label',\n",
        "                                                target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                                class_mode='raw',\n",
        "                                                shuffle=False)\n",
        "\n",
        "    predict_result = model.predict(test_flow_gen, verbose=True)\n",
        "    \n",
        "    predict_result.squeeze()\n",
        "\n",
        "    return predict_result"
      ],
      "metadata": {
        "id": "VIPaYsawIDoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result = predict(model)"
      ],
      "metadata": {
        "id": "0fbL5AjrIPBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측 결과를 csv 파일로 변환"
      ],
      "metadata": {
        "id": "rg8q5VzYII4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_to_csv(predict_result):\n",
        "    result = []\n",
        "\n",
        "    dirs = sorted(os.listdir(TEST_PATH))\n",
        "    img_dir = []\n",
        "    \n",
        "    for i in range(len(dirs)):\n",
        "        imgs = sorted(os.listdir(TEST_PATH + dirs[i] + \"/\"))\n",
        "        imgs = [x for x in imgs if \".jpg\" in x]\n",
        "        \n",
        "        sum = np.sum(predict_result)\n",
        "        mean = np.mean(sum)\n",
        "        \n",
        "        result.append(mean)\n",
        "\n",
        "        for _ in imgs:\n",
        "            img_dir.append(dirs[i])\n",
        "\n",
        "    now = time.localtime()\n",
        "    result_df = pd.DataFrame({'ImageDir': img_dir, 'AvgWeight': result})\n",
        "    FILE_NAME = f\"{now.tm_year}_{now.tm_mon}{now.tm_mday}_{now.tm_hour}_{now.tm_min}_{now.tm_sec}.csv\"\n",
        "    result_df.to_csv(RESULT_PATH + FILE_NAME, index=False, encoding='cp949')\n",
        "\n",
        "    print(\"convert complete\")"
      ],
      "metadata": {
        "id": "RK449HnQIJAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_to_csv(predict_result)"
      ],
      "metadata": {
        "id": "hdC1R2xFIRrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 가중치를 MODEL_PATH로 이동"
      ],
      "metadata": {
        "id": "0iCVGdA0IJUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_weight_dir():\n",
        "    now = time.localtime()\n",
        "    DIR_NAME = f\"{now.tm_year}_{now.tm_mon}{now.tm_mday}_{now.tm_hour}_{now.tm_min}_{now.tm_sec}/\"\n",
        "    os.mkdir(MODEL_PATH + DIR_NAME)\n",
        "\n",
        "    files = os.listdir(MODEL_PATH)\n",
        "\n",
        "    for f in files:\n",
        "        ext = os.path.splitext(f)[-1]\n",
        "        if ext == \".hdf5\":\n",
        "            shutil.move(MODEL_PATH + f, MODEL_PATH + DIR_NAME + f)\n",
        "\n",
        "    print(f\"weights are saved in {MODEL_PATH + DIR_NAME}\")"
      ],
      "metadata": {
        "id": "oSHu1gAUIaJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "move_weight_dir()"
      ],
      "metadata": {
        "id": "e6lR5O2SIcPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}